Resource management, Chair: Rebecca Isaacs (China Hall) 3:15 PM - 4:30 PMResource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud PlatformsEli Cortez (Microsoft); Anand Bonde (Microsoft Research); Alexandre Muzio (ITA, Brazil); Mark Russinovich, Marcus Fontoura (Microsoft); Ricardo Bianchini (Microsoft Research)Traditional performance cost tradeoff problem61	VM packing61	Understanding and predicting VM behaviorUsing ML in resource management (for workload prediction)61	size study: 1. Small vm with scale out pattern, cpu cores similar to memory, customer prefer small vms.61	Cpu utilization, 61	Most vm has low avg utilization61	large % with high p95 utils020202-->Implies: 61	High utils limit packaging61	Low utils oversubscription61	VM lifetime61	Short VMs dominates 61	Small of VMs last forever020202--> Implies VM scheduling is possibleArchitecture: offline + onlineMultiple ML models (check paper)Case study smart cpu over-subscriptionQ: training examples how many?A: three months of data, 1 month for prediction, recurrence training dailyQ: Do you measure the efficiency of VMs?A: SLAs are about (take this offline)Q: A: No create over subscription. Q: Tail performance? A: RC server as resource prediction rather than major resource managementMittOS: Supporting Millisecond Tail Tolerance with Fast Rejecting SLO-Aware OS InterfaceMingzhe Hao, Huaicheng Li, Michael Hao Tong, Chrisma Pakha, Riza O. Suminto, Cesar A. Stuardo, Andrew A. Chien, Haryadi S. Gunawi (University of Chicago)Tail latency at ms: identify straggler and start backup requires long waitCloning introduce 2x workloadSnitching: does not workSolution: do a fast reject on OS level, so there should be no waitMittOS: SLO aware interface, reject fast, transparent of busynessPrediction is on queue policy-> how many IO? How long will they take:?(prediction is on disk, SSD, and memory)Device type: using MittCFQ, MittCFQ profiling:61	Random seek, random read, collect latencylimitation: FTP invisible, invisible dynamic GCRejection: based on SLO queue policy and device typeQ: the complexity of the queueA: Hash map of key and value to be the timeQ: only io is considered but what about other factors?A: Experiments on CPU but not shown, network is however more complex that requires a global viewQ: SLO how to try?A: 95% of base workloadQ: over rejection is a problem?A: probability of two machines being busy at the same time is rareMonotasks: Architecting for Performance Clarity in Data Analytics FrameworksKay Ousterhout (UC Berkeley); Christopher Canel (Carnegie Mellon University); Sylvia Ratnasamy (UC Berkeley); Scott Shenker (UC Berkeley, ICSI)Performance clarity:Reasoning about performance:61	Difficulty:61	Concurrent tasks are sharing same resources61	Resource are controlled by OS61	Single task can be bottlenecked multiple resource61	Monotasks:61	Split job into monotask that uses one resource61	Dedicated scheduler control contention61	Resource use outside of framework61	No model for performanceEasy to reason about performance by constructing pipelineScheduler can potentially utilize the information for better scheduleLittle decrease in performanceHard to re constructing system to adapt monotasksA: Data skew affect accuracy?A: using the metadata return to Q; combine different monotasks especially memory?A: Memory scheduler is in processed, optimized of locality can be achievedQ: # tasks of one job to get best performance?A: People usually use more tasks than the slots so it didnt become a problem